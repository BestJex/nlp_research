task_type: 'classify'
#task_type: 'match'
#task_type: 'ner'
base:
  path_root: ""
  vocab_file_path: "base/chinese_L-12_H-768_A-12/vocab.txt"
  bert_config_file_path: "base/chinese_L-12_H-768_A-12/bert_config.json"
  init_checkpoint_path: "base/chinese_L-12_H-768_A-12/bert_model.ckpt"

classify:
  path_root: "data/classify/"
  ori_path: "intent.csv"
  train_path: "train.csv"
  test_path: "test.csv"
  predict_path: "test1.csv"
  dict_path: "dict.pkl"
  classes_path: "classes"
  checkpoint_path: "checkpoint"
  model_path: "model.pb"
  use_language_model: False

  maxlen: 40
  embedding_size: 128
  learning_rate: 0.001
  batch_size: 64
  num_epochs: 350
  num_class: 89
  valid_step: 1000 #valid one time every valid_step 
  # embedding: char_embedding, word_embedding, subword_embedding
  embedding: 'char_embedding'
  rand_embedding: True
  # encoder: "text_cnn" #88.6
  # encoder: "cnn"  # 84.7
  loss_type: "softmax_loss"

  config_type: 8
  config: 
    0: {encoder: "fasttext", learning_rate: 0.001}
    1: {encoder: "transformer", learning_rate: 0.0001}
    2: {encoder: "transformer", embedding: 'subword_embedding', learning_rate: 0.0001}
    3: {encoder: "rnn", rnn_type: "gru", embedding: 'char_embedding', learning_rate: 0.001}
    4: {encoder: "rnn", rnn_type: "lstm", embedding: 'char_embedding', learning_rate: 0.001}
    5: {encoder: "attention_rnn", rnn_type: "gru", embedding: 'char_embedding', learning_rate: 0.001}
    6: {encoder: "rcnn", rnn_type: "gru", embedding: 'char_embedding', learning_rate: 0.001}
    7: {encoder: "capsule", caps_type: "A", embedding: 'char_embedding', learning_rate: 0.001, loss_type: "margin_loss"}
    8: {encoder: "bert", learning_rate: 0.00001, use_language_model: True, valid_step: 100}

  mode: "train"  #train\test\test_one
  # mode: "test"  #train\test\test_one
  # mode: "test_one"
  # mode: "predict"
match:
  path_root: "data/match/"
  ori_path: "intent.csv"
  index_path: "index.csv"
  relation_path: "relation.csv"
  test_path: "test.csv"
  dict_path: "dict.pkl"
  classes_path: "classes"
  checkpoint_path: "checkpoint"
  model_path: "model.pb"

  batch_mode: "random"  #(random/supervised)
  #batch_mode: "supervised"

  batch_size: 64
  num_epochs: 15000
  num_class: 89
  learning_rate: 0.0001
  margin: 1
  score_thre: 0.8

  # embedding: 'word_embedding'
  embedding: 'char_embedding'
  # embedding: 'subword_embedding'
  rand_embedding: True
  # rand_embedding: False
  # loss_type: 包括pairwise和pointwise,
  # sim_mode=cross: 对于交互的匹配方法
  # sim_mode=represent: 基于表示的匹配方法
  # loss_type: "pointwise"
  keep_prob: 0.5
  loss_type: "pairwise"
  config_type: 0
  config: 
    0: {encoder: "match_pyramid", sim_mode: "cross", num_output: 1}
    1: {encoder: "abcnn", sim_mode: "cross", num_output: 1, learning_rate: 0.001}
    2: {encoder: "esim", sim_mode: "cross", num_output: 1, learning_rate: 0.001}
    3: {encoder: "drmm", sim_mode: "cross", num_output: 1, learning_rate: 0.1}

    #fine tuned
    10: {encoder: "fasttext", sim_mode: "represent", num_output: 256, learning_rate: 0.0001}
    11: {encoder: "rnn", rnn_type: "gru", sim_mode: "represent", num_output: 256, learning_rate: 0.001}
    12: {encoder: "rnn", rnn_type: "lstm", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    13: {encoder: "rcnn", rnn_type: "lstm", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    14: {encoder: "attention_rnn", rnn_type: "gru", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    15: {encoder: "attention_rnn", rnn_type: "lstm", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    21: {encoder: "text_cnn", sim_mode: "represent", num_output: 256, learning_rate: 0.001}
    22: {encoder: "transformer", sim_mode: "represent", num_output: 256, learning_rate: 0.00001}

  mode: "train"  #train\test\test_one
  #mode: "test_one"  #train\test\test_one
  #mode: "test"  #train\test\test_one



ner:
  path_root: "data/ner/"
  train_path: "train_data"
  test_path: "test_data"
  result_path: "result"
  dict_path: "word2id.pkl"
  classes_path: "classes"
  checkpoint_path: "checkpoint"
  model_path: "model.pb"

  batch_size: 64
  num_epochs: 30
  maxlen: -1
  num_tag: 7
  learning_rate: 0.001
  optimizer_type: "Adam"
  keep_prob: 0.5
  embedding: 'char_embedding'
  rand_embedding: True
  use_crf: True
  num_hidden: 256
  num_layers: 1
  tag2label: {"O": 0, "B-PER": 1, "I-PER": 2, 
              "B-LOC": 3, "I-LOC": 4, "B-ORG": 5, "I-ORG": 6}
  config_type: 0
  config: 
    0: {encoder: "rnn", rnn_type: "bi_lstm"}

  mode: "train"  #train\test\test_one
  #mode: "test_one"  #train\test\test_one
  #mode: "test"  #train\test\test_one

