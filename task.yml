task_type: 'classify'
task_type: 'match'
#task_type: 'ner'
#task_type: 'seq_generate'
#task_type: 'translation'


base:
  path_root: ""
  vocab_file_path: "language_model/chinese_L-12_H-768_A-12/vocab.txt"
  bert_config_file_path: "language_model/chinese_L-12_H-768_A-12/bert_config.json"
  init_checkpoint_path: "language_model/chinese_L-12_H-768_A-12/bert_model.ckpt"

classify:
  path_root: "data/classify/"
  ori_path: "intent.csv"
  train_path: "train.csv"
  test_path: "test.csv"
  #test_path: "train.csv"
  predict_path: "test1.csv"
  dict_path: "dict.pkl"
  classes_path: "classes"
  checkpoint_path: "checkpoint"
  model_path: "model.pb"
  use_language_model: False

  maxlen: 40
  embedding_size: 128
  learning_rate: 0.001
  batch_size: 64
  num_epochs: 350
  num_class: 89
  valid_step: 1000 #valid one time every valid_step 
  thre_score: 0.3
  # embedding_type: char_embedding, word_embedding, subword_embedding
  embedding_type: 'char_embedding'
  embedding_type: 'region_embedding'
  rand_embedding: True
  # encoder_type: "text_cnn" #88.6
  # encoder_type: "cnn"  # 84.7
  loss_type: "softmax_loss"
  #loss_type: "margin_loss"
  #loss_type: "focal_loss"

  config_type: 0
  config: 
    0: {encoder_type: "fasttext", learning_rate: 0.001}
    1: {encoder_type: "transformer", learning_rate: 0.0001}
    2: {encoder_type: "transformer", embedding_type: 'subword_embedding', learning_rate: 0.0001}
    3: {encoder_type: "rnn", rnn_type: "gru", embedding_type: 'char_embedding', learning_rate: 0.001}
    4: {encoder_type: "rnn", rnn_type: "lstm", embedding_type: 'char_embedding', learning_rate: 0.001}
    5: {encoder_type: "attention_rnn", rnn_type: "gru", embedding_type: 'char_embedding', learning_rate: 0.001}
    6: {encoder_type: "rcnn", rnn_type: "gru", embedding_type: 'char_embedding', learning_rate: 0.001}
    7: {encoder_type: "capsule", caps_type: "A", embedding_type: 'char_embedding', learning_rate: 0.001, loss_type: "margin_loss"}
    8: {encoder_type: "bert", learning_rate: 0.00002, use_language_model: True, valid_step: 400}

  mode: "train"  #train\test\test_one
  #mode: "test"  #train\test\test_one
  #mode: "test_one"
  # mode: "predict"
match:
  path_root: "data/match/"
  ori_path: "intent.csv"
  index_path: "index.csv"
  relation_path: "relation.csv"
  test_path: "test.csv"
  dict_path: "dict.pkl"
  classes_path: "classes"
  checkpoint_path: "checkpoint"
  model_path: "model.pb"

  cache_vec_path: "cache.vec.pkl"

  batch_mode: "random"  #(random/supervised)
  #batch_mode: "supervised"

  batch_size: 64
  num_epochs: 15000
  num_class: 89
  maxlen: 40
  embedding_size: 128
  learning_rate: 0.0001
  margin: 1
  score_thre: 0.8
  valid_step: 1000 #valid one time every valid_step 

  use_language_model: False
  # embedding_type: 'word_embedding'
  embedding_type: 'char_embedding'
  # embedding_type: 'subword_embedding'
  rand_embedding: True
  # rand_embedding: False
  # loss_type: 包括pairwise和pointwise,
  # sim_mode=cross: 对于交互的匹配方法
  # sim_mode=represent: 基于表示的匹配方法
  # loss_type: "pointwise"
  #keep_prob: 0.5
  loss_type: "pairwise"
  config_type: 10
  config: 
    0: {encoder_type: "match_pyramid", sim_mode: "cross", num_output: 1}
    1: {encoder_type: "abcnn", sim_mode: "cross", num_output: 1, learning_rate: 0.001}
    2: {encoder_type: "esim", sim_mode: "cross", num_output: 1, learning_rate: 0.001}
    3: {encoder_type: "drmm", sim_mode: "cross", num_output: 1, learning_rate: 0.1}

    #fine tuned
    10: {encoder_type: "fasttext", sim_mode: "represent", num_output: 256, learning_rate: 0.0001}
    11: {encoder_type: "rnn", rnn_type: "gru", sim_mode: "represent", num_output: 256, learning_rate: 0.001}
    12: {encoder_type: "rnn", rnn_type: "lstm", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    13: {encoder_type: "rcnn", rnn_type: "lstm", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    14: {encoder_type: "attention_rnn", rnn_type: "gru", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    15: {encoder_type: "attention_rnn", rnn_type: "lstm", sim_mode: "represent", num_output: 128, learning_rate: 0.001}
    21: {encoder_type: "text_cnn", sim_mode: "represent", num_output: 256, learning_rate: 0.001}
    22: {encoder_type: "transformer", sim_mode: "represent", num_output: 256, learning_rate: 0.00001}
    23: {encoder_type: "bert", sim_mode: "cross", learning_rate: 0.00002, use_language_model: True, valid_step: 20000, num_output: 1}

  mode: "train"  #train\test\test_one
  #mode: "test"  #train\test\test_one
  #mode: "test_one"  #train\test\test_one




ner:
  path_root: "data/ner/"
  train_path: "train_data"
  test_path: "test_data"
  result_path: "result"
  dict_path: "word2id.pkl"
  classes_path: "classes"
  checkpoint_path: "checkpoint"
  model_path: "model.pb"

  batch_size: 64
  epoch_num: 30
  maxlen: -1
  embedding_size: 128
  valid_step: 1000 #valid one time every valid_step 
  num_class: 7
  learning_rate: 0.001
  optimizer_type: "Adam"
  #keep_prob: 0.5
  use_language_model: False
  embedding_type: 'char_embedding'
  rand_embedding: True
  use_crf: True
  num_hidden: 256
  num_layers: 1
  tag2label: {"O": 0, "B-PER": 1, "I-PER": 2, 
              "B-LOC": 3, "I-LOC": 4, "B-ORG": 5, "I-ORG": 6}
  config_type: 1
  config: 
    0: {encoder_type: "rnn", rnn_type: "bi_lstm"}
    1: {encoder_type: "bert", learning_rate: 0.00002, use_language_model: True, valid_step: 400}

  mode: "train"  #train\test\test_one
  #mode: "test_one"  #train\test\test_one
  #mode: "test"  #train\test\test_one

seq_generate:
  path_root: "data/seq_generate/"
  train_path: "tang"
  # train_path: "turing"
  dict_path: "vocab.pkl"

  checkpoint_path: "checkpoint"
  model_path: "model.pb"
  use_language_model: False

  maxlen: 40
  embedding_size: 128
  learning_rate: 0.001
  batch_size: 64
  num_epochs: 350
  valid_step: 1000 #valid one time every valid_step 
  # embedding_type: char_embedding, word_embedding, subword_embedding
  embedding_type: 'char_embedding'
  embedding_type: 'region_embedding'
  rand_embedding: True

  num_hidden: 128
  num_layers: 1
  #for sequence generation, encoder_type only support "rnn" up to now
  config_type: 2
  config: 
    0: {encoder_type: "rnn", rnn_type: "gru", embedding_type: 'char_embedding', learning_rate: 0.005}
    1: {encoder_type: "rnn", rnn_type: "bi_lstm", embedding_type: 'char_embedding', learning_rate: 0.001}
    2: {encoder_type: "rnn", rnn_type: "bi_gru", embedding_type: 'char_embedding', learning_rate: 0.001}

  mode: "train"  #train\test\test_one
  #mode: "test_one"  #train\test\test_one



translation:
  path_root: "data/translation/"
  train_path: "turing"
  dict_path: "vocab.pkl"

  checkpoint_path: "checkpoint"
  model_path: "model.pb"
  use_language_model: False

  maxlen: 40
  embedding_size: 128
  learning_rate: 0.001
  batch_size: 64
  num_epochs: 350
  valid_step: 1000 #valid one time every valid_step 
  # embedding_type: char_embedding, word_embedding, subword_embedding
  embedding_type: 'char_embedding'
  embedding_type: 'region_embedding'
  rand_embedding: True

  num_hidden: 128
  num_layers: 1
  #for translation, encoder_type only support "seq2seq" up to now
  config_type: 1
  config: 
    0: {encoder_type: "seq2seq", rnn_type: 'gru', embedding_type: 'char_embedding', learning_rate: 0.005}
    1: {encoder_type: "seq2seq", rnn_type: 'bi_lstm', embedding_type: 'char_embedding', learning_rate: 0.005}

  mode: "train"  #train\test\test_one
  #mode: "test_one"  #train\test\test_one


